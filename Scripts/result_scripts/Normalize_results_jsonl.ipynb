{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef382c3-4944-4000-8e0d-1d29da5c499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df06b62-acd5-4963-99d7-d9a391b01968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_7460\\3236651198.py:2: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  file = pd.read_json(path_file, lines = True)\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_7460\\3236651198.py:2: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  file = pd.read_json(path_file, lines = True)\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_7460\\3236651198.py:2: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  file = pd.read_json(path_file, lines = True)\n"
     ]
    }
   ],
   "source": [
    "path_file =(r\"C:\\Users\\hp\\Desktop\\ET-Airlines\\result_list\\result.jsonl\")\n",
    "file = pd.read_json(path_file, lines = True)\n",
    "file[\"candidates\"] = file[\"candidates\"].apply(len)\n",
    "file[\"job_title\"] = file[\"job_title\"].str.lower()\n",
    "file[\"date_time\"] = file[\"date_time\"].str.lower()\n",
    "file[\"location\"] = file[\"location\"].str.lower()\n",
    "file[\"announcement\"] = file[\"announcement\"].str.lower()\n",
    "\n",
    "def trainee(title):\n",
    "    if not isinstance(title, str):\n",
    "        return None\n",
    "    is_trainee = title.lower()\n",
    "\n",
    "    # check if the title containes any 'trainee' in it\n",
    "    # If it contains that string then it is announcment for trainee's \n",
    "    # We will create a column called title then we will assign 'trainee' or 'job' applicants\n",
    "    if \"trainee\" in is_trainee:\n",
    "        return \"trainee\"\n",
    "    else:\n",
    "        return \"job\"\n",
    "\n",
    "file[\"position\"] = file[\"job_title\"].apply(trainee)\n",
    "\n",
    "def normalize_titles(title):\n",
    "    if not isinstance(title, str):\n",
    "        return None\n",
    "    \n",
    "    t = title.lower()\n",
    "\n",
    "    # remove location indicators like \"- jigjiga\", \"- hawassa\", etc.\n",
    "    # (because location is already its own column)\n",
    "    t = re.sub(r\"-\\s*[a-z ]+$\", \"\", t)\n",
    "\n",
    "    # remove meaningless prefixes\n",
    "    t = re.sub(r\"\\bet[- ]?sponsored\\b\", \"\", t)\n",
    "    t = re.sub(r\"\\bet\\b\", \"\", t)  # catches \"ET-SPONSORED\" or \"ET \"\n",
    "\n",
    "    # remove trainee/junior/assistant\n",
    "    t = re.sub(r\"\\btrainee\\b\", \"\", t)\n",
    "    t = re.sub(r\"\\bjr\\b\", \"\", t)\n",
    "    t = re.sub(r\"\\bjunior\\b\", \"\", t)\n",
    "    t = re.sub(r\"\\bassistant\\b\", \"\", t)\n",
    "    t = re.sub(r\"\\b(?![ac]\\b)[a-z]\\b\", \"\", t)\n",
    "\n",
    "    # remove applicant language\n",
    "    t = re.sub(r\"\\bapplicant[s]?\\b\", \"\", t)\n",
    "\n",
    "    # remove extra punctuation and spaces\n",
    "    t = re.sub(r\"[^a-z0-9/& ]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    return t\n",
    "\n",
    "file[\"job_title\"] = file[\"job_title\"].apply(normalize_titles)\n",
    "\n",
    "def normalize_announcement(text):\n",
    "    text_lower = text.lower()\n",
    "    if \"interview\" in text_lower:\n",
    "        return \"interview\"\n",
    "    elif \"written\" in text_lower:\n",
    "        return \"written exam\"\n",
    "    elif \"employment\" in text_lower:\n",
    "        return \"employment process\"\n",
    "    elif \"practical\" in text_lower:\n",
    "        return \"practical exam\"\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "file[\"announcement\"]= file[\"announcement\"].apply(normalize_announcement)\n",
    "\n",
    "def normalize_location(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    t = re.sub(r\"[^a-z0-9\\s]\", \" \", text.lower()).strip()\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "\n",
    "    # canonicalize some common city spellings/aliases\n",
    "    aliases = {\n",
    "        \"gonder\": \"gondar\",\n",
    "        \"bahir\": \"bahir dar\",\n",
    "        \"haramaya\": \"harar\",\n",
    "        \"addis\": \"addis ababa\",\n",
    "        \"madda\": \"robe\",\n",
    "        \"goba\": \"robe\",\n",
    "        \"nekemte\": \"nekemte\",\n",
    "        \"wollega\": \"nekemte\",\n",
    "        \"kebridehar\": \"kebri dehar\",\n",
    "        \"kabri\": \"kebri dehar\",\n",
    "        \"dire\": \"dire dawa\",\n",
    "        \"semera\": \"semera\",\n",
    "        \"arbaminch\": \"arba minch\",\n",
    "        \"arba minch\":\"arba minch\",\n",
    "        \"ethiopian\":\"addia ababa\"\n",
    "    }\n",
    "    for a, canon in aliases.items():\n",
    "        if a in t:\n",
    "            return canon\n",
    "        # fallback: return the cleaned token containing known city names\n",
    "    known_cities = [\n",
    "        \"mekelle\",\"dessie\",\"gondar\",\"harar\",\"jigjiga\",\"hawassa\",\"nekemte\",\n",
    "        \"robe\",\"semera\",\"arba minch\",\"bahir dar\",\"dire dawa\",\"gambella\",\n",
    "        \"shashemene\",\"addis ababa\",\"adama\",\"wolkite\",\"ambo\",\"gode\",\"jimma\",\n",
    "        \"assosa\",\"kebri dehar\"\n",
    "    ]\n",
    "    for city in known_cities:\n",
    "        if city in t:\n",
    "            return city\n",
    "\n",
    "    return t\n",
    "\n",
    "file[\"location\"]=file[\"location\"].apply(normalize_location)\n",
    "\n",
    "def clean_date_time(text):\n",
    "    try:\n",
    "        return parser.parse(str(text), fuzzy=True)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "file[\"date_time\"]= file[\"date_time\"].apply(clean_date_time)\n",
    "\n",
    "def map_region(location):\n",
    "    if not isinstance(location, str):\n",
    "        return None\n",
    "    l = location.lower()\n",
    "\n",
    "    region_map = [\n",
    "        ([\"mekelle\"], \"tigray\"),\n",
    "        ([\"wollo\",\"dessie\",\"gondar\",\"bahir dar\"], \"amhara\"),\n",
    "        ([\"nekemte\",\"adama\",\"robe\",\"goba\",\"shashemene\",\"gode\",\"jimma\",\"ambo\",\"kebri dehar\",\"kebridehar\"], \"oromia\"),\n",
    "        ([\"hawassa\"], \"sidama\"),\n",
    "        ([\"addis ababa\",\"ethiopian\"], \"addis ababa\"),\n",
    "        ([\"dire dawa\",\"dire\"], \"dire dawa\"),\n",
    "        ([\"jigjiga\",\"kebri dehar\",\"kebridehar\"], \"somali\"),\n",
    "        ([\"gambella\"], \"gambella\"),\n",
    "        ([\"assosa\"], \"benishangul-gumuz\"),\n",
    "        ([\"harar\",\"haramaya\"], \"harari\"),\n",
    "        ([\"semera\"], \"afar\"),\n",
    "        ([\"arba minch\",\"wolkite\"], \"snnpr\")\n",
    "    ]\n",
    "\n",
    "    for keys, region in region_map:\n",
    "        for key in keys:\n",
    "            if key in l:\n",
    "                return region\n",
    "    return \"other\"\n",
    "file[\"region\"] = file[\"location\"].apply(map_region)\n",
    "\n",
    "path_saving = (r\"C:\\Users\\hp\\Desktop\\practices\\0_Tabelau_analysis_files\\ET_Airlines\\FINAL CSV\\result.csv\")\n",
    "\n",
    "file.to_csv(path_saving, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5a753-d9e3-4815-9bb1-b0ada47c7fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
